LUS-12311 ldiskfs: mballoc cr loops optmisation

Remember cr loop allocation failures and skip the
loops for requests of the same or bigger size.
Reset the values after a tunable number
of loop skips.

Signed-off-by: Alexander Zarochentsev <alexander.zarochentsev@hpe.com>
Change-Id: I70d7198473c3a59358009a33a08db12da0c2510a
Reviewed-on: https://es-gerrit.hpc.amslabs.hpecorp.net/163068
Reviewed-by: Alexey Lyashkov <alexey.lyashkov@hpe.com>
Tested-by: Jenkins Build User <nssreleng@cray.com>
Reviewed-by: Andrew Perepechko <andrew.perepechko@hpe.com>
---
 fs/ext4/ext4.h    |  7 +++++++
 fs/ext4/mballoc.c | 17 ++++++++++++++++-
 fs/ext4/mballoc.h |  3 +++
 fs/ext4/sysfs.c   | 25 +++++++++++++++++++++++++
 4 files changed, 51 insertions(+), 1 deletion(-)

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 68672cb9..bdbf9078 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -1547,6 +1547,13 @@ struct ext4_sb_info {
 	atomic_t s_mb_discarded;
 	atomic_t s_lock_busy;
 
+	/* per cN loop min allocation failure size */
+	long s_mb_cX_failed_size[3];
+	/* and their TTL */
+	long s_mb_cX_failed_ttl[3];
+	/* initial TTL value, 0 to disable the feature */
+	long s_mb_cX_ttl0;
+
 	/* locality groups */
 	struct ext4_locality_group __percpu *s_locality_groups;
 
diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c
index a9abddda..438fbe9f 100644
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@ -2743,6 +2743,14 @@ ext4_mb_regular_allocator(struct ext4_allocation_context *ac)
 repeat:
 	for (; cr < 4 && ac->ac_status == AC_STATUS_CONTINUE; cr++) {
 		ac->ac_criteria = cr;
+
+		if (cr < 3 && sbi->s_mb_cX_failed_size[cr] != 0 &&
+		    ac->ac_g_ex.fe_len >= sbi->s_mb_cX_failed_size[cr]) {
+			if (--(sbi->s_mb_cX_failed_ttl[cr]) > 0)
+				continue;
+			sbi->s_mb_cX_failed_ttl[cr] = 0;
+			sbi->s_mb_cX_failed_size[cr] = 0;
+		}
 		/*
 		 * searching for the right group start
 		 * from the goal value specified
@@ -2822,8 +2830,14 @@ repeat:
 				break;
 		}
 		/* Processed all groups and haven't found blocks */
-		if (sbi->s_mb_stats && i == ngroups)
+		if (sbi->s_mb_stats && i == ngroups) {
 			atomic64_inc(&sbi->s_bal_cX_failed[cr]);
+			if (cr < 3 && (sbi->s_mb_cX_failed_size[cr] == 0 ||
+			    sbi->s_mb_cX_failed_size[cr] > ac->ac_g_ex.fe_len)) {
+				sbi->s_mb_cX_failed_size[cr] = ac->ac_g_ex.fe_len;
+				sbi->s_mb_cX_failed_ttl[cr] = sbi->s_mb_cX_ttl0;
+			}
+		}
 	}
 
 	if (ac->ac_b_ex.fe_len > 0 && ac->ac_status != AC_STATUS_FOUND &&
@@ -3536,6 +3550,7 @@ int ext4_mb_init(struct super_block *sb)
 		sbi->s_mb_c3_blocks =
 			THRESHOLD_BLOCKS(sbi, MB_DEFAULT_C3_THRESHOLD);
 	sbi->s_mb_max_inode_prealloc = MB_DEFAULT_MAX_INODE_PREALLOC;
+	sbi->s_mb_cX_ttl0 = EXT4_MB_FAIL_SIZE_TTL;
 	/*
 	 * The default group preallocation is 512, which for 4k block
 	 * sizes translates to 2 megabytes.  However for bigalloc file
diff --git a/fs/ext4/mballoc.h b/fs/ext4/mballoc.h
index 5b7515c3..1879f654 100644
--- a/fs/ext4/mballoc.h
+++ b/fs/ext4/mballoc.h
@@ -72,6 +72,9 @@
 #define MB_DEFAULT_C2_THRESHOLD		15
 #define MB_DEFAULT_C3_THRESHOLD		5
 
+/* TTL for failed allocation request size */
+#define EXT4_MB_FAIL_SIZE_TTL           100
+
 /*
  * default group prealloc size 512 blocks
  */
diff --git a/fs/ext4/sysfs.c b/fs/ext4/sysfs.c
index d3984c24..e6251284 100644
--- a/fs/ext4/sysfs.c
+++ b/fs/ext4/sysfs.c
@@ -25,6 +25,7 @@ typedef enum {
 	attr_mb_c3_threshold,
 	attr_mb_klta_rate,
 	attr_mb_klta_start,
+	attr_mb_cX_ttl0,
 	attr_session_write_kbytes,
 	attr_lifetime_write_kbytes,
 	attr_reserved_clusters,
@@ -192,6 +193,24 @@ static ssize_t mb_klta_start_store(struct ext4_sb_info *sbi,
 	return count;
 }
 
+static ssize_t mb_cX_ttl0_store(struct ext4_sb_info *sbi,
+				const char *buf, size_t count)
+{
+	unsigned long val;
+	int ret;
+
+	ret = kstrtol(buf, 0, &val);
+        if (ret || val < 0)
+                return -EINVAL;
+
+	sbi->s_mb_cX_ttl0 = val;
+	sbi->s_mb_cX_failed_size[0] = 0;
+	sbi->s_mb_cX_failed_size[1] = 0;
+	sbi->s_mb_cX_failed_size[2] = 0;
+
+	return count;
+}
+
 #define EXT4_ATTR(_name,_mode,_id)					\
 static struct ext4_attr ext4_attr_##_name = {				\
 	.attr = {.name = __stringify(_name), .mode = _mode },		\
@@ -270,6 +289,7 @@ EXT4_ATTR(journal_task, 0444, journal_task);
 EXT4_RW_ATTR_SBI_UI(mb_prefetch, s_mb_prefetch);
 EXT4_RW_ATTR_SBI_UI(mb_prefetch_limit, s_mb_prefetch_limit);
 EXT4_RW_ATTR_SBI_UI(mb_reset_last_group, s_mb_reset_last_group);
+EXT4_ATTR_FUNC(mb_cX_ttl0, 0644);
 
 static unsigned int old_bump_val = 128;
 EXT4_ATTR_PTR(max_writeback_mb_bump, 0444, pointer_ui, &old_bump_val);
@@ -314,6 +334,7 @@ static struct attribute *ext4_attrs[] = {
 	ATTR_LIST(mb_prefetch),
 	ATTR_LIST(mb_prefetch_limit),
 	ATTR_LIST(mb_reset_last_group),
+	ATTR_LIST(mb_cX_ttl0),
 	NULL,
 };
 
@@ -393,6 +414,8 @@ static ssize_t ext4_attr_show(struct kobject *kobj,
 		return snprintf(buf, PAGE_SIZE, "%llu\n",
 				(unsigned long long)
 				atomic64_read(&sbi->s_resv_clusters));
+	case attr_mb_cX_ttl0:
+		return snprintf(buf, PAGE_SIZE, "%ld\n", sbi->s_mb_cX_ttl0);
 	case attr_inode_readahead:
 	case attr_pointer_ui:
 		if (!ptr)
@@ -464,6 +487,8 @@ static ssize_t ext4_attr_store(struct kobject *kobj,
 		return mb_klta_rate_store(sbi, buf, len);
 	case attr_mb_klta_start:
 		return mb_klta_start_store(sbi, buf, len);
+	case attr_mb_cX_ttl0:
+		return mb_cX_ttl0_store(sbi, buf, len);
 	}
 	return 0;
 }
-- 
2.45.2
